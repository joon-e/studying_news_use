<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Studying News Use with Computational Methods</title>
    <meta charset="utf-8" />
    <meta name="author" content="Julian Unkel" />
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link rel="stylesheet" href="cdm_theme/theme.css" type="text/css" />
    <link rel="stylesheet" href="cdm_theme/theme_fonts.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: left, bottom, title-slide, social_media, title-slide

# Studying News Use with Computational Methods
## Data Collection in R, Part I: Collecting Social Media Data
### Julian Unkel
### University of Konstanz
### 2021/05/10

---


# Agenda

.pull-left[As we have seen in the previous sessions, social media play a key role for users' news consumption. 

By their design, they record human behavior (and thus news use behavior) as _digital traces_ of users engaging and interacting (clicking on news links, liking news posts, writing comments on said posts) with social media posts.

In this session, we will deal with common approaches to collect digital trace data from social media.
]

--

.pull-right[Our agenda today:

- API requests
  - Basics
  - Calling APIs with R
- API wrapper packages
  - Basics
  - Example: Querying the Twitter API with `rtweet`
- Facepager
- Social listening services
  - CrowdTangle
  - Other commercial options
]

---
class: middle

# API requests

---

# Basics: HTTP requests

.pull-left[
Think of accessing data on web servers (e.g., by opening a web site in a browser) via **HTTP** (Hypertext Transfer Protocol) as ordering a package via mail:

- First, we place an order with our client, for example by typing an URL into a browser (*Request*)
- The server sends our client a package (*Response*), consisting of two parts:
  - _Header_: Sort of like the packing slip; contains lots of meta information, for example whether our package was delivered succesfully
  - _Body_: The actual content of the package, for example an HTML file

]

.pull-right[
![HTTP requests in the clientâ€“server model](img/1/client-server.png)
HTTP requests in the client-server model
]

---

# Basics: HTTP methods &amp; status codes


There are several different **request methods**, most importantly:

- _GET_: Request data retrieval
- _POST_: Request sending (=posting) data (e.g, web forms)

--

Response headers contain three-digit **status** codes that tell us if everything went okay or what went wrong. Most importantly:

- _2xx_: Success! We usually want code _200_, telling us that everything is OK
- _4xx_: Oh no, client error! This means: The problem is caused by the client (i.e., us). You have probably already encountered these:
  - *403*: Forbidden - client is not allowed to access the requested resource
  - *404*: Not found - client requested a resource that is not available on the server
- *5xx*: Oh no, server error! For example, _503_ (service unavailable) tells us that the server is (currently) to busy to handle our request. 

---

# Basics: Writing HTTP requests in R

We can write our own HTTP requests in R using the `httr` package. Let's install it if we haven't done so already:


```r
install.packages("httr")
```

--

After loading the package, we can use functions named after the request methods to send HTTP requests. Let's request your [SEDS home page](https://www.wiwi.uni-konstanz.de/studium/master-of-science/seds/).


```r
library(httr)
seds_resp &lt;- GET("https://www.wiwi.uni-konstanz.de/studium/master-of-science/seds/")
```



--

The response is a list object containing the 'whole package'. Let's first take a look at the status code:


```r
status_code(seds_resp)
```

```
## [1] 200
```

Everything went OK!

---

# Basics: Writing HTTP requests in R

We can now investigate the body - the actual content - of our response object:


```r
content(seds_resp)
```

```
## {html_document}
## &lt;html lang="de"&gt;
## [1] &lt;head&gt;\n&lt;meta http-equiv="Content-Type" content="text/html; charset=UTF-8 ...
## [2] &lt;body class="faculty"&gt;\n\n    &lt;nav class="anchormenu" aria-label="Sprungm ...
## [3] &lt;script src="https://www.wiwi.uni-konstanz.de/typo3temp/assets/compressed ...
## [4] &lt;script src="https://www.wiwi.uni-konstanz.de/typo3temp/assets/compressed ...
## [5] &lt;script type="text/javascript"&gt;\n/*&lt;![CDATA[*/\n/*TS_inlineFooter*/\n\t\t ...
## [6] &lt;script type="text/javascript"&gt;\r\n  var _paq = _paq || [];\r\n  /* track ...
## [7] &lt;script type="text/x-mathjax-config"&gt;\r\n      MathJax.Hub.Config({\r\n   ...
## [8] &lt;script type="text/javascript" src="/MathJax/MathJax.js?config=TeX-AMS_HT ...
```

The first lines tell us that we have successfully requested an `html_document`. We will deal with working with HTML documents in the next session. But you can already see the first level of contents of the HTML file, namely a `&lt;head&gt;` with meta information, the `&lt;body&gt;` containing all the text of the website (not to be confused with the header and the body of the *response*), and various `&lt;script&gt;`s.

---

# Basics: What's in a URL?

We access resources on the web by providing the corresponding **URL** (Uniform Resource Locator). Let's take a closer look:

&lt;span class="vibrant-1"&gt;`https`&lt;/span&gt;`://`&lt;span class="vibrant-2"&gt;`www.google.de`&lt;/span&gt;&lt;span class="vibrant-3"&gt;`/search`&lt;/span&gt;&lt;span class="vibrant-4"&gt;`?q=seds`&lt;/span&gt;

--

- &lt;span class="vibrant-1"&gt;Scheme&lt;/span&gt;: The scheme specifies the protocol that we are using (HTTPS is a secure version of HTTP)
- &lt;span class="vibrant-2"&gt;Domain&lt;/span&gt;: The domain name indicates the web server that is being requested
- &lt;span class="vibrant-3"&gt;Path&lt;/span&gt;: The path points to the specific resource on the web server, just like the folder structure on your computer. It can include the file name (e.g., `/path/to/page.html`), but on web pages, this is usually handled on the server side.
- &lt;span class="vibrant-4"&gt;Parameters&lt;/span&gt;: Web servers may accept parameters in a `key=value` combination to dynamically provide content for a specific resource. They are separated from the path by a single `?`. Multiple parameters can be linked by `&amp;` (e.g., `?key1=value1&amp;key2=value2`).

In the above example, we are thus requesting the resource at path `/search` with the parameter `q` set to `seds` of the domain `www.google.de` via the HTTPS protocol: [https://www.google.de/search?q=seds](https://www.google.de/search?q=seds)

--

We can add other parameters to change the output: [https://www.google.de/search?q=seds&amp;start=10](https://www.google.de/search?q=seds&amp;start=10)

---

# Basics: JSON

Web-APIs usually do not return HTML files, but more structured data, most often in the **JSON** (JavaScript Object Notation, pronounced as in ["Jason and The Argonauts"](https://www.ecma-international.org/wp-content/uploads/ECMA-404_2nd_edition_december_2017.pdf)). This open, human-readable and flexible text format is able to represent various (also hierarchical and nested) data structures in attribute-value pairs. We will deal with JSON files soon, but the example from [Wikipedia](https://en.wikipedia.org/wiki/JSON) probably already tells you all the basics you need to know:


```json
{
  "firstName": "John",
  "lastName": "Smith",
  "isAlive": true,
  "age": 27,
  "address": {
    "streetAddress": "21 2nd Street",
    "city": "New York",
    "state": "NY",
    "postalCode": "10021-3100"
  },
  "phoneNumbers": [
    {
      "type": "home",
      "number": "212 555-1234"
    },
    ...
```

---

# APIs

**APIs** (application programming interfaces) are interfaces of software applications to communicate (e.g., share data) with other applications. 

In our context, the term usually references Web APIs, interfaces of web applications/servers using request-response systems. 

--

All Web APIs are different and thus require some engagement with the (hopefully helpful) documentation: 
- access requirements
- endpoints and parameters
- response data structures

--

But all Web APIs are the same:
- we write an HTTP request to the API URL
- the API responds by providing the requested data (usually in JSON, XML, or CSV)

---

# APIs: Authentication &amp; Rate limits

Access to APIs is regulated in many different ways, for example:

- Open (can be called without any authentication)
- Username/password
- API key (often passed as a URL parameter)
- OAuth (a protocol for generating user- oder session-specific authentication tokens)

In all but the first case, this requires (often reviewed or even paid) registration.

--

APIs usually manage access by setting **rate limits**, defining how many calls a user can make within a given time period. Exceeding the rate limit may result in:

- Request errors (e.g., 429 Too Many Requests)
- Request throttling
- Fees (in commercial APIs)

---

# APIs: Endpoints &amp; Parameters

Most APIs offer several **endpoints** for specific actions. Endpoints are thus a combination of an URL path and an HTTP request method. 

For example, some endpoints of the Twitter API v2, using the base URL `https://api.twitter.com` are:

- `GET /2/tweets`: Get information about tweets
- `GET /2/users/:id/tweets`: Get tweets of the Twitter user with the id `:id`
- `POST /2/users/:id/likes`: Like a tweet on behalf of the Twitter user with the id `:id`

--

Calls to endpoints are then usually specified further by providing **parameters**, either as URL parameters or, for example when using the POST method, in the request body.

- For `GET /2/tweets`, we would add a list of tweet IDs to our call by adding the parameter `ids` (e.g, `GET https://api.twitter.com/2/tweets?ids=id1,id2,id3`)
- For `POST /2/users/:id/likes`, we would add the id of the target tweet in the request body in JSON format (e.g., `{"tweet_id": "id1"}`)

---

# APIs: Social Media

[Programmable Web](https://www.programmableweb.com/) provides an overview of about 25,000 APIs you may want to use.

Common social media APIs are:

- **Twitter API** (https://developer.twitter.com/en/docs/twitter-api)
  - Access to Twitter tweets, timelines, profiles, etc.
  - *Will I get access?* Likely, through the [Academic Research track](https://developer.twitter.com/en/solutions/academic-research)
- **Facebook Graph API** (https://developers.facebook.com/docs/graph-api)
  - Acces to Facebook posts, comments, profiles, etc.
  - *Will I get access?* Unlikely (but wait for the rest of the session)
- **Facebook Ad Library API** (https://www.facebook.com/ads/library/api)
  - Access to political Facebook ads (content, reach, spendings, etc.) 
  - *Will I get access?* Very likely
- **Instagram Graph API** (https://developers.facebook.com/docs/instagram-api)
  - Access to Instagram posts, profiles, etc.
  - *Will I get access?* Unlikely (but wait for the rest of the session)
- **Reddit API** (https://www.reddit.com/dev/api/)
  - Reddit submissions, comments, etc.
  - *Will I get access?* Actually haven't tried it (because see next slide)
  
---

# The Pushshift API

**Pushshift** is a privately maintained, open Reddit dataset, ingesting Reddit content in real time. For technical details, see the paper [The Pushshift Reddit Dataset](https://arxiv.org/pdf/2001.08435.pdf).

The dataset is accessible, among other pathways, via a public, open API: https://api.pushshift.io, documented at https://github.com/pushshift/api.

Main advantages over the 'real' Reddit API:

- No authentication required
- Larger response object limits
- Very forgiving rate limits

Drawbacks:

- Unclear state of development, incomplete documentation
- Some issues with deleted posts
- Likely coverage issues

---

# Calling the Pushshift API with R

Let's write our first API call! The base URL of the Pushshift API is https://api.pushshift.io, so we might want to store this for easier retrieval:


```r
ps_base &lt;- "https://api.pushshift.io"
```

--

As seen in the [documentation](https://github.com/pushshift/api), the API currently offers two endpoints, both for `GET` methods:

- `/reddit/search/comment`: Searching individual comments
- `/reddit/search/submission`: Searching submissions

Let's store them as well:


```r
ps_comment &lt;- "/reddit/search/comment"
ps_submission &lt;- "/reddit/search/submission"
```

---

# Calling the Pushshift API with R

The `GET()` function of `httr` offers several arguments to construct a response from the different parts of the call URL. We can use the `url` argument to add the base URL (domain), define the path using the `path` arguments, and add several parameters by passing a named list of key/value pairs to the argument `query`.

In the following, we call the submission endpoint of the API, searching for the latest 100 submissions in the [r/news](https://www.reddit.com/r/news) subreddit that contain the word `"biden"` in the submission title:


```r
ps_resp &lt;- GET(url = ps_base,
               path = ps_submission,
               query = list(subreddit = "news",
                            title = "biden",
                            size = 100)
               )
```



---

# Calling the Pushshift API with R

Let's take a look:


```r
ps_resp
```

```
## Response [https://api.pushshift.io/reddit/search/submission?subreddit=news&amp;title=biden&amp;size=100]
##   Date: 2021-04-29 15:27
##   Status: 200
##   Content-Type: application/json; charset=UTF-8
##   Size: 478 kB
## {
##     "data": [
##         {
##             "all_awardings": [],
##             "allow_live_comments": false,
##             "author": "paulfromatlanta",
##             "author_flair_css_class": null,
##             "author_flair_richtext": [],
##             "author_flair_text": null,
##             "author_flair_type": "text",
## ...
```

---

# Calling the Pushshift API with R

We can 'unpack' the response body by using the `content()` function:


```r
ps_content &lt;- content(ps_resp, type = "application/json")
str(ps_content, max.level = 1)
```

```
## List of 1
##  $ data:List of 100
```

--

Further moving through the list levels, we can access information about the individual entries:


```r
ps_data &lt;- ps_content$data
ps_data[[1]]$title
```

```
## [1] "Biden administration bans menthol cigarettes"
```

(Your results may vary as I'm using a cached response in this presentation.)

---

# Calling the Pushshift API with R

Using some [Tidyverse](https://www.tidyverse.org/) functions - specifically, from the `purrr` package for functional programming - we can quickly transform the response to a rectangular dataframe:


```r
library(tidyverse)
fields &lt;- c("id", "title", "created_utc", "permalink", "url")
ps_data %&gt;% 
  map_dfr(magrittr::extract, fields)
```

```
## # A tibble: 100 x 5
##    id     title              created_utc permalink             url              
##    &lt;chr&gt;  &lt;chr&gt;                    &lt;int&gt; &lt;chr&gt;                 &lt;chr&gt;            
##  1 n16pfm Biden administrat~  1619709687 /r/news/comments/n16~ https://www.cbsn~
##  2 n15u35 Biden Tax Plan Le~  1619707290 /r/news/comments/n15~ https://www.wsj.~
##  3 n14vxm Biden Seeks Shift~  1619704570 /r/news/comments/n14~ https://www.nyti~
##  4 n14tp9 President Biden p~  1619704385 /r/news/comments/n14~ https://abcnews.~
##  5 n148h3 Indian-Americans ~  1619702541 /r/news/comments/n14~ http://news.meim~
##  6 n13nyb Women Make Histor~  1619700751 /r/news/comments/n13~ https://www.scra~
##  7 n12jtq &amp;amp;#x27;White S~  1619696752 /r/news/comments/n12~ https://www.news~
##  8 n11j6p At 100 Days, Bide~  1619692414 /r/news/comments/n11~ https://newsnati~
##  9 n112x0 Ex-Trump aide Ste~  1619690395 /r/news/comments/n11~ https://apple.ne~
## 10 n10wv1 Joe Biden Unveils~  1619689554 /r/news/comments/n10~ https://www.rayz~
## # ... with 90 more rows
```

---

# Calling the Pushshift API with R

**Exercise 1: Write your own call:** 
Try to obtain the *first* 50 posts that were posted in German-language subreddit [r/de](https://www.reddit.com/r/de). Consult the documentation for help on the necessary parameters: https://github.com/pushshift/api

&lt;center&gt;&lt;img src="https://media.giphy.com/media/LmNwrBhejkK9EFP504/giphy.gif"&gt;&lt;/center&gt;

---

# APIs: Iteration &amp; Pagination

If an API call matches more results than can be returned with a single response, we need an iteration mechanism to retrieve all results. For example, if the call matches 500 results and the response object limit is 100, we need to make (at least) 5 calls to retrieve all results. Keep rate limits in mind when iterating over results!

Most APIs provide one or more of the following forms of pagination:

- **Pages**: Results are spread over pages (e.g., results 1 to 100 on page 1, 101 to 200 on page 2). We can then iterate over results by simply adding 1 to the page number (e.g., by adding the query parameter `page=page_num`) in each successive call.
- **Keys**: Results are ordered by ascending/descending keys (e.g., Tweet IDs). We can then iterate over results by retrieving the minimum/maximum key of each call and requesting results below/above said key in the next call.
- **Timestamps**: Results are ordered by UNIX timestamps or [DIN ISO 8601](https://en.wikipedia.org/wiki/ISO_8601) date formats. We can then iterate over results by retrieving the minimum/maximum timestamp of each call and requesting results before/after said timestamp in the next call (but beware that multiple results can have the same timestamp).
- **Cursors**: Results are spread over pages, but single pages are identified by an opaque cursor (i.e., usually a seemingly random sequence of characters) instead of integer numbers. We can then iterate over results by retrieving the cursor for the next/previous page which should be provided in the response.

---

# Calling the Pushshift API with R

**Exercise 2: Pagination:** 
Try to obtain the *latest* 200 comments posted in the [r/politics](https://www.reddit.com/r/politics) subreddit that contain the phrase "lol". Consult the documentation for help on the necessary parameters: https://github.com/pushshift/api

&lt;center&gt;&lt;img src="https://media.giphy.com/media/LmNwrBhejkK9EFP504/giphy.gif"&gt;&lt;/center&gt;

---

# Exercise solutions

Exercise 1:


```r
ex1_resp &lt;- GET(url = ps_base,
                path = ps_submission,
                query = list(subreddit = "de",
                             sort = "asc")
                )
```

---

# Exercise solutions

Exercise 2:


```r
# Get first 100 comments
ex2_resp_1 &lt;- GET(url = ps_base,
                  path = ps_comment, 
                  query = list(q = "lol",
                               subreddit = "politics",
                               size = 100))
ex2_data_1 &lt;- content(ex2_resp_1)$data

# Extract timestamp of last result
last_comment_timestamp &lt;- tail(ex2_data_1, 1)[[1]]$created_utc

ex2_resp_2 &lt;- GET(url = ps_base,
                  path = ps_comment, 
                  query = list(q = "lol",
                               subreddit = "politics",
                               size = 100,
                               before = last_comment_timestamp))
```

(Note that to make sure we do not miss any comments posted at the same time, we could add +1 to the `last_comment_timestamp` and then filter out eventual duplicates.)

---

class: middle
# Thanks

Credits:
- Slides created with [`xaringan`](https://github.com/yihui/xaringan)
- Title image by [Tracy Le Blank / Pexels](https://www.pexels.com/de-de/foto/person-mit-iphone-die-den-ordner-fur-soziale-netzwerke-anzeigt-607812/)
- Icons by [Bootstrap](https://icons.getbootstrap.com/)
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
