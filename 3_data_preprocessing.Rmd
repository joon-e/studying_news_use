---
title: "Studying News Use with Computational Methods"
subtitle: "Text Preprocessing in R"
author: "Julian Unkel"
institute: "University of Konstanz"
date: "2021/06/14"
output:
  xaringan::moon_reader:
    css: [default, "cdm_theme/theme.css", "cdm_theme/theme_fonts.css"]
    lib_dir: libs
    nature:
      ratio: "16:9"
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      titleSlideClass: ["left", "bottom", "title-slide", "alphabet"]
---

# Agenda

.pull-left[After having collected texts, we now need to bring them into a form with which statistical language models are able to work. In practice, this means reducing our text to features (i.e., predictor variables) used in statistical models represented by numbers.

Such **preprocessing** steps includes ways to transform text in numbers, clean, edit and reduce the amount of features, and cast them into the data structures ready for text models.
]

--

.pull-right[Our agenda today:

- Text representation in R
  - Terminology
  - The `tidytext` approach
  - The `quanteda` approach
- Basic text preprocessing
  - Tokenization
  - Document-feature matrices
  - Feature reduction: Trimming, stemming, stopword removal
- Advanced text preprocessing & annotation
  - Lemmatization
  - Part-of-speech tagging
  - Named entity recognition
  - Weighting
  - Word embeddings
]

---
class: middle

# Terminology

---

# Terminology

In automated content analysis, we are analysing **corpora** of **documents**:

- **Document**: A single item of text as the basic unit of analysis, for example an article, a social media post, or a speech transcript
- **Corpus**: A structured collection of documents

--

Documents consist of a text string and, optionally, additional meta information:

- **Feature**: Any document property or characteristic used in the models. Think of features as predictors or explanatory/independent variables. For example, counts of individual words, but also any kind of document meta information may be used as a feature.
- **Token**: Any meaningful unit (sub-string) of a text (string). For example, we may _tokenize_ a document into words, sentences, letters, etc.
- **n-gram**: Any contiguous sequence of tokens. Thus, 1-grams (*unigrams*) may be single words ("It's, "peanut", "butter", "jelly", "time"), 2-grams (*bigrams*) sequences of two words ("It's peanut" "peanut butter", "butter jelly", "jelly time"), etc.

---

# The `tidytext` approach

The [`tidytext`](https://cran.r-project.org/web/packages/tidytext/vignettes/tidytext.html) package was created as an Tidyverse extension to apply tidy data principles to textual data and text analysis. Thus, a corpus is represented as a tibble, with documents as rows and document variables as columns. 

```{r eval=FALSE}
install.packages("tidytext")
library(tidytext)
```

```{r echo=FALSE}
library(tidyverse)
library(tidytext)
```

--

The sample data `guardian_sample_100.rds` on ILIAS contains a sample of 100 each articles published by The Guardian in 2020 in the news and sports section, respectively, including several additional variables (e.g., publication date) already in this format. Load it now:

```{r eval=FALSE}
guardian_tibble <- readRDS("data/guardian_sample_100.rds")
```

```{r echo=FALSE}
guardian_tibble <- readRDS("temp/guardian_sample_100.rds")
```

---

# The `tidytext` approach

```{r}
guardian_tibble
```

---

# The `quanteda` approach

The most prominent package for automated content analysis in R is called [`quanteda`](https://quanteda.io/) (*Qu*antitative *An*alysis of *Te*xtual *Da*ta):

```{r eval=FALSE}
install.packages("quanteda")
library(quanteda)
```

```{r echo=FALSE, message=FALSE}
library(quanteda)
```

--

`quanteda` uses it's own data type called `corpus` to represent text corpora. The `corpus()` function can be used to create corpora from a variety of other data types, including dataframes/tibbles.

If creating a corpus from a dataframe/tibble, use the `docid_field` and `text_field` arguments to select id and text variables, respectively:

```{r}
guardian_corpus <- corpus(guardian_tibble, docid_field = "id", text_field = "body")
```

---

# The `quanteda` approach

```{r}
guardian_corpus
```

---

# The `quanteda` approach

Use `docvars()` to access the document variables:

```{r}
as_tibble(docvars(guardian_corpus))
```

---

# Converting between `tidytext` and `quanteda`

Both packages work fine together. Use `tidytext::tidy()` at any time to convert `quanteda` objects to `tidytext`-style tibbles:

```{r}
tidy(guardian_corpus)
```

---
class: middle

# Basic text preprocessing

---

# Tokenization

**Tokenization** describes the process of splitting texts into individual tokens, which can then be used as features in a text analysis model. In quantitative text analysis, we usually want to split into single words, which is thus the default option for most tokenizers.

--

In `tidytext`, the function is called `unnest_tokens()`. This creates a column with the single tokens and (by default) removes the text input column (in our case, `body`):

```{r}
guardian_tibble_tokenized <- guardian_tibble %>% 
  unnest_tokens(word, body) 
```

The resulting tibble now has one row per word. Note that by default, this removes punctuation and converts all words to lowercase.

---

# Tokenization

```{r}
guardian_tibble_tokenized
```

---

# Tokenization

In `quanteda`, we can use the `tokens()` function on our corpus object:

```{r}
guardian_tokens <- guardian_corpus %>% 
  tokens()
guardian_tokens
```

---

# Tokenization

Note that `tokens()` by default does neither convert to lowercase nor removes punctuations. To get to the same result as with `tidytext`, we have to be more explicit:

```{r}
guardian_tokens <- guardian_corpus %>% 
  tokens(remove_punct = TRUE) %>% 
  tokens_tolower()

guardian_tokens
```

---

# Tokenization

Tokenization options depend on the project and research interest at hand. Both functions also allow you to use other tokenizers, for example from the `tokenizers` package. For example, when tokenizing tweets you may use special tokenizers that preserve hashtags, mentions, and URLs.

--

In practice, I find the following steps to be a good default options:

- Converting to lowercase
- Removing punctuation, numbers, symbols, URLs, and separators

```{r}
guardian_tokens <- guardian_corpus %>% 
  tokens(remove_punct = TRUE, remove_numbers = TRUE,
         remove_symbols = TRUE, remove_url = TRUE,
         remove_separators = TRUE) %>% 
  tokens_tolower()
```

--

In the follwing, we will continue mainly with `quanteda`, but will return to `tidytext` later on.

---

# Tokenization

We can now also create various n-grams of choice by using `tokens_ngrams()` on our tokens object. For example, for bigrams only:

```{r}
guardian_tokens %>% 
  tokens_ngrams(2)
```

---

# Tokenization

Unigrams, bigrams, and trigrams:

```{r}
guardian_tokens %>% 
  tokens_ngrams(1:3)
```

---

# Document-feature matrices

The typical input for most quantitative text analysis methods is called a **DFM** (*D*ocument-*f*eature *m*atrix), with documents in rows and features in columns. Most often, we will use token (i.e., word) counts as features. 

--

We can construct a DFM from our tokens object with `dfm()`:

```{r}
guardian_dfm <- guardian_tokens %>% 
  dfm()
```

---

# Document-feature matrices

```{r}
guardian_dfm
```

--

- In total, our sample contains 16,489 different words (thus: 16,489 features and the same number of columns)
- The DFM's *sparsity* is the proportion of cells with a value of zero.
- Note that our docvars are still accessible via `docvars()`. As such, we could simply column-bind them to the matrix to use (some of) these variables as additional features, which may be particularly useful for (supervised) classification methods. However, in the following sessions, we will mainly work with word features only.

---

# Feature reduction

Even with as little as 200 articles, we have quite a large matrix with already several million cells. This can easily grow to several hundred million cells even with medium-sized corpora. This can both increase the computational load and make it harder to find meaningful, predictive features. 

--

It is thus usually a good idea to reduce the dimensionality of the DFM by removing unnecessary features. The three most common steps are:

- *Trimming* by removing very uncommon and/or very common features because they usually have little to no discriminative or predictive value
- *Stemming* words to their word stem, so for example singular and plural forms of the same word are represented in the same feature
- Removing common functional words (*stopwords*) like conjunctions and articles because, again, they usually hhave little to no discriminative or predictive value

---

# Feature reduction: Trimming

Use `dfm_trim()` to trim features from a DFM. You can trim both by term frequency (how often does the feature appear across all documents) and document frequency (in how many documents does the feature appear), and both by absolute and relative values (among others).

--

For example, the following code removes all features that appear less than 5 times across all documents and all features that appear in more than 75% of all documents:

```{r}
guardian_dfm %>% 
  dfm_trim(min_termfreq = 5, termfreq_type = "count",
           max_docfreq = 0.75, docfreq_type = "prop")
```

---

# Feature reduction: Stemming

Use `dfm_wordstem()` to conduct word stemming. This uses the [Snowball](https://snowballstem.org/) stemming algorithm, which is currently available for 26 languages. Use the `language` argument to set the language (default: `"english"`), and `SnowballC::getStemLanguages()` to get a list of available languages.

--

```{r}
guardian_dfm %>% 
  dfm_wordstem(language = "english")
```

For example, note `coalit`, which has replaced `coalition's` (and all other forms of `coalition`) and now has count of 9. 

---

# Feature reduction: Stopword removal

Use `dfm_remove()` to remove any features given in a character vector from the DFM. Stopword removal is thus pretty much a form of trimming by providing a list of features to remove. 

`quanteda` (via the `stopwords` package) provides several lists of stopwords for various languages. For example, the default list of english stopwords, also from the Snowball project, contains 175 stopwords:

```{r}
stopwords("english")
```

---

# Feature reduction: Stopword removal

For example, note that `the` is gone (and that the sparsity of the matrix actually increased) after stopword removal:

```{r}
guardian_dfm %>% 
  dfm_remove(stopwords("english"))
```

---

# Feature reduction

Some general tips on feature reduction:

- There is no single best way that applies to all projects. 
- It is often advisable to try out different feature reduction steps and check their effects on the model outcome. 
- However, a good, robust text model should also be somewhat resistant to small changes in feature reductions.
- Order matters! For example, when using both stemming and trimming, stem first, then trim.
- Stemming and stopword removal can also be applied to token objects before creating the DFM (with `tokens_wordstem()` and `tokens_remove()`, respectively).

---

# Basic text preprocessing

**Exercise 1: Preprocessing**

`aoc_tweets.csv` (on ILIAS) contains 783 tweets by Alexandria Ocasio-Cortez made in 2021, as obtained by Twitter's Academic API. 

- Load the tweets into R and create a quanteda corpus object
- Tokenize, convert to lowercase and remove punctuation, emojis, numbers, and URLs
- Create a DFM, remove english stopwords, the retweet indicator "RT", #hashtags and @mentions (hint: look at the `?dfm_remove()` documentation).
- Bonus points: Check the DFM and the most common features (`topfeatures()`) to identify further problematic features (and propose solutions)

<center><img src="https://media.giphy.com/media/LmNwrBhejkK9EFP504/giphy.gif"></center>

---

class: middle

# Advanced text preprocessing & annotation

---

# Using `spacyr`

More sophisticated preprocessing requires pre-trained language models. The Python module `spaCy` provides several pre-trained models for a variety of languages, capable of such tasks as lemmatization, part-of-speech tagging, and named entity recognition. 

--

We can make use of `spaCy`'s preprocessing pipelines with the R package `spacyr`, but this still needs `spaCy` (and the language models) to be installed:

```{eval=FALSE}
pip install spacy
python -m spacy download en_core_web_sm
```

--

After installing both `spaCy` and `spacyr`, we can then initialize the language models as follows:

```{r message=FALSE}
library(spacyr)
spacy_initialize(model = "en_core_web_sm")
```

---

# Lemmatization

**Lemmatization**, stemming's fancy sibling, groups inflected word forms to their common dictionary form, lemma. This means that, unlike with stemming, also irregular forms of words can be grouped together (i.e., `"is"` and `"are"` are both lemmatized as `"be"`). However, lemmatization thus also needs to identify the word meaning in a sentence, and as such requires full text (and not just a bag of words).

--

With a pre-trained model, the process itself is quite simple. We use `spacyr`'s `spacy_parse()` function on a text corpus and set `lemma = TRUE`:

```{r}
guardian_lemma <- spacy_parse(guardian_corpus, lemma = TRUE,
                              pos = FALSE, entity = FALSE)
```

---

# Lemmatization

```{r eval=FALSE}
guardian_lemma
```

```{r echo=FALSE}
as_tibble(guardian_lemma)
```

For example, note that `"is"` in line 10 has been correctly lemmatized to `"be"`.

---

# Lemmatization

From here, we can continue with `quanteda` functions by converting the word list into a token object:

```{r}
as.tokens(guardian_lemma, use_lemma = TRUE)
```

---

# Part-of-speech tagging

**Part-of-speech tagging** (POS tagging) identifies the corresponding part of speech for each word, for example:

- `NOUN`: noun
- `VERB`: verb
- `ADJ`: adjective
- `PROPN`: proper noun

etc.

--

POS tagging is more relevant for more linguistics-focused models, but may still be useful for statistical approaches as well, for example for disambiguation purposes. We can again use `spacy_parse()`, this time setting `pos = TRUE`:

```{r}
guardian_pos <- spacy_parse(guardian_corpus, lemma = FALSE,
                            pos = TRUE, entity = FALSE)
```

---

# Part-of-speech tagging

```{r eval=FALSE}
guardian_pos
```

```{r echo=FALSE}
as_tibble(guardian_pos)
```

---

# Named entity recognition

Finally, we can use `spacyr` to extract named entities in a corpus. Named entities may include:

- `ORG`: organisations
- `PERSON`: persons
- `NORP`: nationalities, religious or political groups
- `GPE`: geo-political entities, i.e., countries, cities, states
- `FAC`: facilities like buildings, airports, etc.

and several more.

--

Again, we use `spacy_parse()`, this time setting `entity = TRUE`:

```{r}
guardian_ner <- spacy_parse(guardian_corpus, lemma = FALSE,
                            pos = FALSE, entity = TRUE)
```

---

# Named entity recognition

```{r eval=FALSE}
guardian_ner %>% 
  filter(entity != "")
```

```{r echo=FALSE}
as_tibble(guardian_ner) %>% 
  filter(entity != "")
```

Note that `_B` identifies the beginning of an entity, `_I` an "inside" part of an entity.

---

# Using `spacyr`

Running `spaCy` language models consumes lots of memory, so remember to shut down those background processes after all annotation has been completed:

```{r}
spacy_finalize()
```

---

# Weighting

Going back to DFM's and `quanteda`, we may improve model performance by weighting the features in the DFM. The general form to weight DFMs in `quanteda` is to use the function `dfm_weight()`. For example, to use the feature proportion per document instead of absolute frequencies, set `scheme = "prop"`:

```{r}
guardian_dfm %>% 
  dfm_weight(scheme = "prop")
```

---

# Weighting

One common form of weighting is to use the **term frequency - inverse document frequency** (*tf-idf*) statistic. The tf-idf value increases with the number of times a word appears in a document, and is offset with the number of times the respective word appears in documents accross the whole corpus. As such, it seeks to reflect how important/distinctive a word is for a specific document.

In `quanteda`, use `dfm_tfidf()` to apply the tf-idf weighting to a DFM object. For example, note that `"the"` has a value of 0 even though it is the most frequent word in the first document:

```{r}
guardian_dfm %>% 
  dfm_tfidf()
```

---

# Word embeddings

With DFMs, we follow the bag-of-words model, that is, we choose to ignore word order and all semantical and syntactical relationships between words (and this actually works fine for most purposes). However, alternative ways of representing text as numbers exist that seek to preserve such connections between lexical units. 

With **word embeddings** (also *word vectors*), we encode words in high-dimensional numeric vectors so that words that have little distance in vector space are more similar in meaning than words that have larger distances.

There are complex, pre-trained word embedding models ready to use, such as [`word2vec`](https://cran.r-project.org/web/packages/word2vec/), but for demonstration purposes, we will create our own word embeddings. 

The following steps are based on chapter 5 of the great book ["Supervised machine learning for text analysis in R"](https://smltar.com/embeddings.html) by Emil Hvitfeldt and Julia Silge.

---

# Word embeddings

First, note that word embeddings are usually trained on a huge corpus of documents, which also makes them very computationally expensive. For demonstration purposes, let's use a larger sample of the Guardian corpus, containing 10,000 articles:

```{r}
guardian_larger <- readRDS("temp/guardian_sample_2020.rds")
guardian_larger
```

---

# Word embeddings

These 10,000 articles consist of 7,824,081 words in total, and 117,188 unique words:

```{r}
guardian_larger_tokens <- guardian_larger %>% 
    unnest_tokens(word, body)

guardian_larger_tokens %>% 
    count(word, sort = TRUE)
```

---

# Word embeddings

First, we nest all documents into their own tibbles, as we want to compute word associations based on their co-occurence within the same document. For example, the row of the first document now contains a nested tibble containing all 698 words as single rows of said document:

```{r}
guardian_nested <- guardian_larger_tokens %>% 
    select(id, word) %>% 
    nest(words = word)
guardian_nested
```

---

# Word embeddings

One way to measure word associations is by looking at how often they appear together and how often they appear alone in predefined windows (i.e., sequences of `n` words). The *pointwise mutual information* (*PMI*) measures this association computing the logarithmn of the probability of finding two words together in a given window, and dividing this by the probability of finding each word alone. The higher the PMI, the more likely those two words appear together.

--

We need some additional packages to efficiently compute this measures for all two-word associations in a given window in our corpus:

```{r}
library(slider) # Creating sliding windows
library(widyr)  # Wide-matrix processing
library(furrr)  # Parallel computing
```

---

# Word embeddings

The [aforementioned book](https://smltar.com/embeddings.html#understand-word-embeddings-by-finding-them-yourself) provides a function to create word windows of a given size:

```{r}
slide_windows <- function(tbl, window_size) {
    skipgrams <- slider::slide(
        tbl, 
        ~.x, 
        .after = window_size - 1, 
        .step = 1, 
        .complete = TRUE
    )
    
    safe_mutate <- safely(mutate)
    
    out <- map2(skipgrams,
                1:length(skipgrams),
                ~ safe_mutate(.x, window_id = .y))
    
    out %>%
        transpose() %>%
        pluck("result") %>%
        compact() %>%
        bind_rows()
}
```

---

# Word embeddings

Let's create word windows of size 8. This is the computationally expensive part of the procedure and can easily run for several hours, depending on the size of the corpus, even on fairly powerful hardware.

Note that the window size is crucial here: a small window captures only close associations and thus focuses on functionally similar words, whereas larger windows capture more thematic information. However, the larger the window, the higher the computational load.

```{r eval=FALSE}
plan(multisession)  ## for parallel processing

guardian_windows <- guardian_nested %>%
    mutate(words = future_map(words, slide_windows, 8L))
```

```{r echo=FALSE}
guardian_windows <- readRDS("temp/guardian_windows.rds")
```

---

# Word embeddings

The result now contains *all* windows of 8 sequential words for each document. For example, the first document contains 698 words, and now 5,528 windows of 8 words:

```{r}
guardian_windows
```

---

# Word embeddings

Some more data transformations - we unnest our nested tibbles and unite the document and window id variables into one variable:

```{r}
guardian_windows_united <- guardian_windows %>% 
    unnest(words) %>% 
    unite(window_id, id, window_id)
```

---

# Word embeddings

Window `1_1` identifies the first 8 word window of the first document, window `1_2` the second 8 word window of the second document (note that this second window begins with the second word of the first window), etc.:

```{r}
guardian_windows_united
```

---

# Word embeddings

We can now compute the *PMI* for each two-word association in a given window using `pairwise_pmi()`:

```{r warning=FALSE}
guardian_pmi <- guardian_windows_united %>% 
    pairwise_pmi(item = word, feature = window_id)
```

---

# Word embeddings

For each word, we now have the PMI of said word appearing with every other word in a 8-word window across the whole corpus. This results in 23,670,248 values for 117,188 unique words:

```{r}
guardian_pmi
```

---

# Word embeddings

Now for the actual word embeddings. To reduce the dimensionality of our word matrix (currently 23,670,248 cells), we apply a matrix factorization algorithmn called *singular value decomposition* (*SVD*) that factors our large initial matrix into a set of smaller matrices, the amount of which corresponds to the dimensionality of the vector space. This reduces the size of our initial word matrix to `n_unique_words * n_dimensions`:

```{r eval=FALSE}
guardian_word_vectors <- guardian_pmi %>%
    widely_svd(
        item1, item2, pmi,
        nv = 100, maxit = 1000
    )
```

```{r}
guardian_word_vectors <- readRDS("temp/guardian_word_vectors.rds")
```

---

# Word embeddings

The resulting tibble now contains 100 values per word, giving the vector position of each word in each of the 100 dimensions. This way, we have reduced the size of our initial word matrix from 23,670,248 to 11,718,800 (117,188 unique words * 100 dimenions):

```{r}
guardian_word_vectors
```

---

# Word embeddings

Where to go from here? For example, we can compute the cosine similarity between words to see how close/distant they are from each other (and thus, how closely they are related in our corpus) in the vector space. Luckily, the [book](https://smltar.com/embeddings.html#exploring-cfpb-word-embeddings) also provides a helpful function for this:

```{r}
nearest_neighbors <- function(df, token) {
    df %>%
        widely(
            ~ {
                y <- .[rep(token, nrow(.)), ]
                res <- rowSums(. * y) / 
                    (sqrt(rowSums(. ^ 2)) * sqrt(sum(.[token, ] ^ 2)))
                
                matrix(res, ncol = 1, dimnames = list(x = names(res)))
            },
            sort = TRUE,
            sparse = TRUE
        )(item1, dimension, value) %>%
        select(-item2)
}
```

---

# Word embeddings

Let's take a look at Joe Biden:

```{r}
nearest_neighbors(guardian_word_vectors, "biden")
```

---

# Word embeddings

Or at Jürgen Klopp:

```{r}
nearest_neighbors(guardian_word_vectors, "klopp")
```

---

# Word embeddings

Who is closer to Washington?

```{r}
nearest_neighbors(guardian_word_vectors, "biden") %>% 
  filter(item1 == "washington")
```

```{r}
nearest_neighbors(guardian_word_vectors, "klopp") %>% 
  filter(item1 == "washington")
```

---

class: middle

# Exercise solutions

---

# Exercise solutions

**Exercise 1: Preprocessing**

We can use `read_csv()` (or the base R equivalent `read.csv`) to read in the CSV file:

```{r eval=FALSE}
aoc_tweets <- read_csv("data/aoc_tweets.csv",
                       col_types = c(id = col_character()))
```

```{r echo=FALSE}
aoc_tweets <- read_csv("temp/aoc_tweets.csv",
                       col_types = list(id = col_character()))
```

Note that when using Twitter data (and other social media data), it is advisable to explicitly read numeric IDs as character, as longer numeric IDs may be too long for double precision.

---

# Exercise solutions

We can use the Tweet `id` for the document id; the tweet text is stored in the `text` column:

```{r}
aoc_corpus <- corpus(aoc_tweets, docid_field = "id", text_field = "text")
aoc_corpus
```

---

# Exercise solutions

Hashtags and mentions are preserved by default. Emojis are symbols and are thus removed with `remove_symbols = TRUE`:

```{r}
aoc_tokens <- aoc_corpus %>% 
  tokens(remove_punct = TRUE, 
         remove_symbols = TRUE,
         remove_numbers = TRUE,
         remove_url = TRUE) %>% 
  tokens_tolower()
```

---

# Exercise solutions

Remember that we lowercased all features, thus the retweet indicator `"RT"` can be removed with `"rt"`:

```{r}
aoc_dfm <- aoc_tokens %>% 
  dfm() %>% 
  dfm_remove(c("rt", stopwords("english"))) %>% 
  dfm_remove(pattern = "@*") %>% 
  dfm_remove(pattern = "#*")
aoc_dfm
```

---

# Exercise solutions

In the DFM, we can already see `"u.s"`, which is the result from tokenizing `"U.S."`. We may want to change this sequence of characters (`"U.S."`) to `"US"` beforehand, for example with `stringr::str_replace_all()`:

```{r eval=FALSE}
aoc_tweets %>% 
  mutate(text = str_replace_all(text, "U\\.S\\.", "US"))
```

However, this would also create some ambiguity with `"us"`.

---

# Exercise solutions

Let's look at the top features for further problems:

```{r}
topfeatures(aoc_dfm)
```

- `"amp"` is the remains of the HTML entity for the ampersand sign, `&amp;`. This (changing some special characters to their HTML entity) is one annoying quirk of the Twitter API. We may just remove this "by hand" (`dfm_remove("amp")`). More generally, the most reliable option to deal with HTML entities is to decode them in the text beforehand `textutils::HTMLdecode()`.
- `"w"` is the remains of `"w/"` (abbreviation for "with"). We may want to remove this by hand, or, more conveniently, use `dfm_select(min_nchar = 2)` to remove all one-letter words from our DFM.

---

class: middle
# Thanks

Credits:
- Slides created with [`xaringan`](https://github.com/yihui/xaringan)
- Title image by [Susan Holt Simpson / Unsplash](https://unsplash.com/photos/Rd01U0tPmQI)
- Coding cat gif by [Memecandy/Giphy](https://giphy.com/gifs/memecandy-LmNwrBhejkK9EFP504)